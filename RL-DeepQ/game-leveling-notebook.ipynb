{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9491801",
   "metadata": {},
   "source": [
    "# 强化学习+深度学习 实现关卡平衡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c41147",
   "metadata": {},
   "source": [
    "## 下载实验代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517814f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/SageMaker/frozen-lake\n",
    "curl https://s3.amazonaws.com/sagemaker-us-east-1-537534971119/frozen-lake.tar.gz > frozen-lake.tar.gz\n",
    "tar xvzf frozen-lake.tar.gz -C ~/SageMaker/frozen-lake/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd17f0e",
   "metadata": {},
   "source": [
    "## 安装冰雪奇缘湖游戏环境以及依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ~/SageMaker/frozen-lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3595e8",
   "metadata": {},
   "source": [
    "安装完毕后需要重启kernel生效，\n",
    "引入所需依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c097699",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    HyperparameterTuner,\n",
    "    ContinuousParameter,\n",
    "    IntegerParameter,\n",
    ")\n",
    "\n",
    "from frozen_lake import (\n",
    "    Level, LeveledFrozenLake,\n",
    "    play_level, train, DeepQConfig,\n",
    "    get_state, DeepQNetwork, moving_average,\n",
    "    play_manually, get_test_level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd5711",
   "metadata": {},
   "source": [
    "## 运行游戏，先玩一下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985a126",
   "metadata": {},
   "source": [
    "使用OpenAI Gym 环境引入FrozenLakeEnv环境用于测试游戏。\n",
    "\n",
    "openAI Gym\n",
    "https://github.com/openai/gym\n",
    "Gym 是一个开源 Python 库，通过提供用于在学习算法和环境之间进行通信的标准 API 以及一组符合该 API 的标准环境，用于开发和比较强化学习算法。自发布以来，Gym 的 API 已成为这样做的现场标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543ca51",
   "metadata": {},
   "source": [
    "### 初始化游戏环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11287db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "env = FrozenLakeEnv()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3a5c0",
   "metadata": {},
   "source": [
    "左上角的“S”代表“开始”，右下角的“G”代表“目标”。 “F”和“H”分别代表“冻结”和“洞”。游戏的想法是导航到目标而不会掉入冰洞。这将是微不足道的，除非冰很滑，使您的动作不确定。\n",
    "\n",
    "与环境交互的最重要的概念是状态、动作和奖励。只要板子被认为是固定的（稍后您将使用动态板子），状态只是光标的位置。环境对象将其存储在 s 属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653214e",
   "metadata": {},
   "source": [
    "当您向右和向下移动时，状态会增加。\n",
    "\n",
    "动作是代理对下一步做什么的决定。在这个游戏中，有四种可能的动作：向左、向下、向右和向上。这四个动作分别用整数 0、1、2 和 3 表示。\n",
    "\n",
    "环境的最后一个重要概念是奖励。在这个游戏中，达到目标的奖励为 1.0，所有其他步骤的奖励为 0.0。在训练期间，代理的目标是找到一个最大化奖励的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(is_slippery=False)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9853a",
   "metadata": {},
   "source": [
    "关闭滑动模式并采取一些确定性操作的步骤来感受 API。在一个新单元格中，创建一个关闭滑动模式的新环境并渲染它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab754ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "env.step(DOWN)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c117d",
   "metadata": {},
   "source": [
    "为方向定义常量，使用 step() 方法执行操作并重新渲染板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c6c10",
   "metadata": {},
   "source": [
    "### 使用API玩一下游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(DOWN)\n",
    "env.step(DOWN)\n",
    "env.step(RIGHT)\n",
    "env.step(RIGHT)\n",
    "env.step(DOWN)\n",
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e465505",
   "metadata": {},
   "source": [
    "现在您已经了解了环境的运作方式，最好手动玩几次游戏以了解游戏的运作方式。虽然目标是训练智能体自动玩游戏，但对游戏有一些直觉会帮助你制定有效的训练策略。尽管在关闭滑行模式时策略很明显，但对代理行为的非确定性响应改变了方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4109292",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 注意需要使用notebook 才可以显示wiget，使用jupuyter notebook lab 按钮显示不了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ecde5",
   "metadata": {},
   "source": [
    "### 通过图形化界面玩一下游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be739c",
   "metadata": {},
   "source": [
    "禁止冰块滑倒模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(is_slippery=false)\n",
    "play_manually(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da415cf",
   "metadata": {},
   "source": [
    "启用冰块滑倒模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(is_slippery=True)\n",
    "play_manually(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7544ab0",
   "metadata": {},
   "source": [
    "到目前为止，已经有两种游戏模式：滑和不滑。滑倒模式非常困难而且违反直觉，优势后莫名其妙的改变指令的方向。这样的滑倒模式意义不大。默认情况下，你可以用来改变《冰雪奇缘湖》游戏难度的唯一办法是创建不同的棋盘。但是为了创建多个关卡，最好也能够调整犯错的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6dfc9d",
   "metadata": {},
   "source": [
    "* 使用随机难度，增加犯错的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_level = LeveledFrozenLake.random(0.2)\n",
    "play_manually(random_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130a742",
   "metadata": {},
   "source": [
    "这些关卡比非滑模式关卡难得多，但比 OpenAI Gym 中的默认游戏要容易得多（后者犯错的概率为 67％）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c6e73",
   "metadata": {},
   "source": [
    "## 训练一个机器人"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06f120",
   "metadata": {},
   "source": [
    "### 随机代理（未经训练的机器人）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b1cb3",
   "metadata": {},
   "source": [
    "现在，你应该创建一个可以自动播放关卡的随机代理。这将作为您可以期望从学习的代理获得的性能的基准 — 学习的代理应该优于随机代理。\n",
    "\n",
    "frozen-lake 软件包中的 play_level () 函数是一个帮助程序，允许代理玩《冰雪奇缘湖》游戏的某个关卡\n",
    "\n",
    "* 该函数有两个参数：env 和 get_action ()。env 参数是冰冻湖环境。另一个参数是可调用对象 —— 一个定义游戏行为的函数。get_action () 可调用对象将接受一个环境并返回一个操作。\n",
    "\n",
    "* play_level () 函数重置环境，然后执行操作，直到游戏完成 或 代理完成 100 个步骤（以先到者为准）。然后它返回的奖励将是 0.0 或 1.0，具体取决于是否赢了该等级。\n",
    "\n",
    "* 要评估随机代理，你需要一个测试级别和一个 get_action () 可调用对象，它在每个步骤都返回一个随机动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28365d43",
   "metadata": {},
   "source": [
    "构建随机代理进行测试\n",
    "定义随机行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(env: FrozenLakeEnv) -> int:\n",
    "    \"\"\"Choose a random action\"\"\"\n",
    "    return np.random.randint(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249c201",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "play_level(get_test_level(), random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f3660",
   "metadata": {},
   "source": [
    "使用随机代理，玩10000次游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random agent on the same level 10,000 times\n",
    "np.random.seed(1)\n",
    "n_attempts = 10000\n",
    "test_level = get_test_level()\n",
    "rewards = [\n",
    "    play_level(test_level, random_action)\n",
    "    for _ in range(n_attempts)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba03990",
   "metadata": {},
   "source": [
    "现在计算随机代理的获胜百分比："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dad2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rewards) / n_attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c834f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96145071",
   "metadata": {},
   "source": [
    "## 训练一个有智慧的代理（机器人）\n",
    "为了训练这个机器人我们需要将当前的状态信息向量化，这里通过一个矩阵来表示，每一个cell表示当前的状态 0表示冻结，1表示洞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e89e7",
   "metadata": {},
   "source": [
    "请记住，Frozen Lake 环境的状态是 [0, 15] 范围内的整数，表示光标在 4x4 板上的位置。如果看板是固定的，这很有效，因为要对下一个操作做出正确的决定，你唯一需要的信息就是当前位置。\n",
    "\n",
    "* 但是，如果同一个代理要玩多个不同的棋盘，那么代理需要有关注两部分信息：板的配置和光标的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b951cc",
   "metadata": {},
   "source": [
    "状态信息可以很容易通过数组或更高维度的张量。为了简单起见，在本教程中，state通过一维数组表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc42723",
   "metadata": {},
   "source": [
    "* 这样board将永远是 4x4的矩阵。你还知道左上角（位置 0）始终是 'S' 表示开始，右下角（位置 15）始终是 'G' 表示目标。因为这两个单元格是固定的，所以你不需要把这些单元格放在状态数组中。\n",
    "* 还有 14 个单元，它们可以是 “F” 表示冻结，也可以是 “H” 表示洞。由于状态数组需要是数字，因此使用 0 表示冻结，使用 1 表示从左向右移动的 H（按行主要顺序）。\n",
    "\n",
    "使用两个 one-hot 向量来存储光标的位置：一个用于光标行，另一个用于列。要将单个整数位置转换为两个 one-hot 向量，请获取整数位置的行 (m) 和列 (n)。一个one-hot向量将是 4x4 单位矩阵的第 n 行和第 n 行。\n",
    "\n",
    "在 frozen-lake 软件包中有一个名为 get_state () 的函数，它在给定状态下为游戏板生成一个完整的状态数组。看看 frozen_lake/state.py 看看它是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_level = get_test_level()\n",
    "test_level.render()\n",
    "get_state(test_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82787349",
   "metadata": {},
   "source": [
    "### 本地训练一个机器人（策略网络）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253a0ed",
   "metadata": {},
   "source": [
    "开始新游戏\n",
    "对于游戏中的每一步：\n",
    "* 通过策略网络运行游戏状态以获取操作\n",
    "* 执行操作并获取新状态\n",
    "* 使用当前奖励（或缺少奖励）更新保单网络\n",
    "* 游戏完成后，打破循环\n",
    "* 训练运行的剧集数量可配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44bd2b",
   "metadata": {},
   "source": [
    "策略神经网络由两个线性层组成，在第一层之后激活一个 relU，在第二层之后激活一个 softmax。softmax 层输出一个大小为 4 的数组（四个操作中的每个操作一个元素）。最佳操作应与最高输出相对应。但是，在训练和模拟游戏过程中，最好引入一些随机性以防止代理卡住。\n",
    "\n",
    "策略网络是在 frozen_lake/model.py 中定义的 —— 看看它是如何工作的。当然，有可能使网络更深入、更复杂。但是对于简单的游戏来说，保持网络规模较小是个好主意，因为游戏很简单，小型网络的训练速度更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd7b4f",
   "metadata": {},
   "source": [
    "如果您按照上述的高级程序训练此网络，则很难学习如何很好地发挥冰冻湖的作用的策略。强化学习往往不稳定，并且有一些修改对于它在实践中很好地发挥作用非常重要：体验重放和目标网络。\n",
    "\n",
    "在实际更新网络之前，体验重放会跟踪几个步骤的奖励。目标网络是策略网络的克隆，更新频率较低。\n",
    "\n",
    "这两个修改的实现都基于 PyTorch 站点的教程。查看 frozen_lake/memory.py 和目标网络中体验重播的内存，并更新 frozen_lake/train.py 中的步进逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1974c",
   "metadata": {},
   "source": [
    "####  使用DeepQ网络训练\n",
    "* DeepQ网络介绍：https://towardsdatascience.com/beating-video-games-with-deep-q-networks-7f73320b9592\n",
    "* Pytorch深度学习介绍：https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebed502",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DeepQConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4eeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_policy, reward = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c15c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(reward))\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('episode');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf89e1f",
   "metadata": {},
   "source": [
    "你可以看到，即使进行了体验重播和目标网络修改，奖励仍然有些不稳定。这是使用超参数搜索扩大训练范围以便找到效果最佳的策略的一个很好的理由。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd9192",
   "metadata": {},
   "source": [
    "现在尝试之前的标准测试级别上的策略网络并打印结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b37bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n_attempts = 10000\n",
    "test_level = get_test_level()\n",
    "rewards = [\n",
    "    play_level(test_level, local_policy.learned_action)\n",
    "    for _ in range(n_attempts)\n",
    "]\n",
    "sum(rewards) / n_attempts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd90824",
   "metadata": {},
   "source": [
    "获胜百分比应高于之前的随机代理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b4ade",
   "metadata": {},
   "source": [
    "## 使用Sagemaker进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15520710",
   "metadata": {},
   "source": [
    "由于学习策略的性能不稳定 —— 不足以估计人类的表现，因此最好将训练扩展到跨不同超参数的多次尝试。SageMaker 让这变得非常简单。首先，您将向单个训练作业发送训练脚本，然后使用超参数调整功能搜索超参数值。\n",
    "\n",
    "要在 SageMaker 上训练作业，您需要一个脚本作为训练容器的入口点。SageMaker 会将超参数作为命令行参数传入。因此，例如，如果你的脚本名为 main.py，并且你想要传入 target_update 值 5 作为超参数，SageMaker 将在训练容器中运行命令 python main.py —target_update 5。\n",
    "\n",
    "脚本需要训练模型并将其保存到特定位置的磁盘中。然后，SageMaker 会将模型上传到 S3，在那里它可以在本地或托管终端节点中使用。\n",
    "\n",
    "在本教程中，main.py 脚本已经编写完成，它位于带有 setup.py 的顶级冷冻湖文件夹中。要在 SageMaker 上将其作为训练作业运行："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e28bc8",
   "metadata": {},
   "source": [
    "集成训练代码到Sagemaker使用BYOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point='main.py',\n",
    "    source_dir='/home/ec2-user/SageMaker/frozen-lake/',\n",
    "    framework_version='1.2.0',\n",
    "    train_instance_type='ml.m5.large',\n",
    "    train_instance_count=1,\n",
    "    role=get_execution_role(),\n",
    "    py_version='py3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d01d74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34859b41",
   "metadata": {},
   "source": [
    "下载模型并解压\n",
    "什么是 PTH 文件？PTH 文件大多属于 PyTorch。PTH 是使用 PyTorch 进行机器学习的数据文件。PyTorch 是一个基于 Torch 库的开源机器学习库。它主要由 Facebook 人工智能研究小组开发。如何打开 PTH 文件。你需要像 PyTorch 这样的合适软件来打开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886939c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $estimator.model_data ./\n",
    "!tar xvzf model.tar.gz\n",
    "!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352097c",
   "metadata": {},
   "source": [
    "策略网络权重现在将位于。/policy.pth。\n",
    "\n",
    "在新单元格中，创建一个新的策略网络并将权重加载到内存中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_policy = DeepQNetwork(config.n_state_features, config.n_actions)\n",
    "sagemaker_policy.load_state_dict(torch.load('policy.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df430a17",
   "metadata": {},
   "source": [
    "在测试级别评估性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb59441",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n_attempts = 10000\n",
    "test_level = get_test_level()\n",
    "rewards = [\n",
    "    play_level(test_level, sagemaker_policy.learned_action)\n",
    "    for _ in range(n_attempts)\n",
    "]\n",
    "sum(rewards) / n_attempts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28df1b",
   "metadata": {},
   "source": [
    "性能应该与 local_policy 网络相同，因为所有随机种子都是固定的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f76d30",
   "metadata": {},
   "source": [
    "## 超参数优化\n",
    "大约需要20分钟左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bea697",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name='MaxReward',\n",
    "    metric_definitions=[\n",
    "        dict(\n",
    "            Name='MaxReward',\n",
    "            Regex='MaxReward=([0-9\\\\.]+)',\n",
    "        )\n",
    "    ],\n",
    "    hyperparameter_ranges=dict(\n",
    "        target_update=IntegerParameter(10, 500),\n",
    "        epsilon_start=ContinuousParameter(0.25, 0.75),\n",
    "    ),\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13995719",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit()\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = (\n",
    "    estimator.output_path +\n",
    "    tuner.best_training_job() +\n",
    "    '/output/model.tar.gz'\n",
    ")\n",
    "\n",
    "!aws s3 cp $model_path ./\n",
    "!tar xvzf model.tar.gz\n",
    "!rm model.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5945c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_policy = DeepQNetwork(config.n_state_features, config.n_actions)\n",
    "tuned_policy.load_state_dict(torch.load('policy.pth'))\n",
    "\n",
    "n_attempts = 10000\n",
    "rewards = [\n",
    "    play_level(test_level, tuned_policy.learned_action)## 原workshop有误，参数应该为test_level，文档中是test_env \n",
    "    for _ in range(n_attempts)\n",
    "]\n",
    "sum(rewards) / n_attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ea7bd",
   "metadata": {},
   "source": [
    "## 按难度排序关卡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e78bbb",
   "metadata": {},
   "source": [
    "创建十个新关卡，并在每个级别上运行学习过的代理 10,000 次。然后存储难度（即 1-win_百分比）、出错的概率和冰洞的数量。按难度对关卡进行排序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "levels = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    level_config = Level.random(config.p_mistake_draw)\n",
    "    level = LeveledFrozenLake(level_config)\n",
    "    win_precentage = sum(\n",
    "        play_level(level, tuned_policy.learned_action)\n",
    "        for _ in range(n_attempts)\n",
    "    ) / n_attempts\n",
    "    n_holes = (np.array(list(''.join(level_config.board))) == 'H').sum()\n",
    "    levels.append(dict(\n",
    "        difficulty=1-win_precentage,\n",
    "        p_mistake=level_config.p_mistake,\n",
    "        n_holes=n_holes,\n",
    "        level=level,\n",
    "    ))\n",
    "\n",
    "levels = sorted(levels, key=lambda l: l['difficulty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b931ffc",
   "metadata": {},
   "source": [
    "这将需要大约十分钟的时间才能运行，因为代理程序必须为每个步骤调用策略网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb258d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level_df = pd.DataFrame(levels)\n",
    "level_df = level_df.sort_values('difficulty')## 难度= （1-对应的就是通关率【win_precentage 】）\n",
    "level_df[['difficulty', 'p_mistake', 'n_holes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411daaf0",
   "metadata": {},
   "source": [
    "难度= （1-对应的就是通关率【win_precentage 】） 也就是说对应级别的难度很低，通关率很高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92828598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
